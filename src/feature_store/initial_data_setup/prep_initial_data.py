"""
This code creates features and target files 
to be ingested by feature store. The data 
is sourced "Bena345/cdc-diabetes-health-indicators"
in Hugging Face, which is generated by 
generate_initial_data.py script.
"""

#################################
import os
import sys
from datetime import datetime

import pandas as pd
from datasets import load_dataset

sys.path.insert(0, os.getcwd() + "/src/")
from pathlib import PosixPath

from feature_store.utils.config import Config
from training.utils.path import DATA_DIR

#################################


def main(config_yaml_abs_path: str, data_dir: PosixPath):
    """Prepares initial raw dataset to be ingested by feature store.

    config_yaml_abs_path (str): absolute path to config.yml file, which
        includes dataset preprocessing configuration.

    Returns:
        Saves the three splits in local path.
    """

    print(
        """\n
    ----------------------------------------------------------------
    --- Preparing Features & Target for Feature Store Starts ...
    ----------------------------------------------------------------\n"""
    )

    # Import data preprocessing config params
    config = Config(config_path=config_yaml_abs_path)

    # Specify variable types and data source from config file
    RAW_DATASET_SOURCE = config.params["data"]["params"]["raw_dataset_source"]
    PRIMARY_KEY = config.params["data"]["params"]["pk_col_name"]
    CLASS_COL_NAME = config.params["data"]["params"]["class_col_name"]
    date_col_names = config.params["data"]["params"]["date_col_names"]
    datetime_col_names = config.params["data"]["params"]["datetime_col_names"]
    num_col_names = config.params["data"]["params"]["num_col_names"]
    cat_col_names = config.params["data"]["params"]["cat_col_names"]
    required_input_col_names = (
        [PRIMARY_KEY]
        + date_col_names
        + datetime_col_names
        + num_col_names
        + cat_col_names
        + [CLASS_COL_NAME]
    )

    #################################
    # Import train and test sets
    dataset = load_dataset(RAW_DATASET_SOURCE)
    train_set = dataset["train"].to_pandas()
    test_set = dataset["test"].to_pandas()

    # Append both sets to form raw dataset for training
    raw_dataset = pd.concat([train_set, test_set], axis=0)
    raw_dataset = raw_dataset[required_input_col_names]

    # Save features and target in a separate parquet files
    # Note: this is meant for patient entity in feature store.
    raw_features = raw_dataset.drop([CLASS_COL_NAME], axis=1, inplace=False)
    raw_features["event_timestamp"] = datetime.now()
    raw_features.to_parquet(
        data_dir / "raw_dataset_features.parquet",
        index=False,
    )

    raw_target = raw_dataset[[PRIMARY_KEY] + [CLASS_COL_NAME]].copy()
    raw_target["event_timestamp"] = datetime.now()
    raw_target.to_parquet(
        data_dir / "raw_dataset_target.parquet",
        index=False,
    )

    print("Training features and target were saved in feature store.")


# python ./src/feature_store/initial_data_setup/prep_initial_data.py ./config/feature_store/config.yml
if __name__ == "__main__":
    main(config_yaml_abs_path=sys.argv[1], data_dir=DATA_DIR)
